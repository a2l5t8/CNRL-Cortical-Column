{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from conex import *\n",
    "from pymonntorch import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from conex.helpers.filters import DoGFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image,normal=False):\n",
    "    plt.axis(\"off\")\n",
    "    if(normal):\n",
    "        plt.imshow(image,cmap='gray',vmin=0,vmax=255)\n",
    "    else:\n",
    "        plt.imshow(image,cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_crop_interspace(inp_width, inp_height, window_width, window_height):\n",
    "    x1 = window_width//2\n",
    "    x2 = (inp_width - 1) - (window_width//2)\n",
    "    y1 = window_height//2 \n",
    "    y2 = (inp_height - 1) - (window_height//2)\n",
    "\n",
    "    center_x = random.randint(x1, x2)\n",
    "    center_y = random.randint(y1, y2)\n",
    "    center_coordinates = [center_x, center_y]\n",
    "    top_left_x = center_x - (window_width//2)\n",
    "    top_left_y = center_y - (window_height//2)\n",
    "    top_left_coordinates = [top_left_x, top_left_y]\n",
    "    coordinates = [center_coordinates, top_left_coordinates]\n",
    "\n",
    "    return coordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "MNIST_ROOT = \"./MNIST\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Width = 24\n",
    "Input_Height = 24\n",
    "Crop_Window_Width = 10\n",
    "Crop_Window_Height = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 100\n",
    "crop_iteration = 2\n",
    "\n",
    "dataset_directory_path = \"./first_step\"\n",
    "\n",
    "transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels = 1), # not necessary\n",
    "    Conv2dFilter(DoGFilter(size = 5, sigma_1 = 4, sigma_2 = 1, zero_mean=True, one_sum=True).unsqueeze(0).unsqueeze(0)),\n",
    "    SqueezeTransform(dim = 0),\n",
    "    SimplePoisson(time_window = time_window , ratio = 2),\n",
    "])\n",
    "\n",
    "\n",
    "dataset = MNIST(root=MNIST_ROOT, train=True, download=False, transform=transformation)\n",
    "first_class = dataset.data[dataset.targets == 4]\n",
    "second_class = dataset.data[dataset.targets == 9]\n",
    "two_class_dataset = torch.cat((first_class, second_class), dim=0)\n",
    "dataset_size = first_class.shape[0] + second_class.shape[0]\n",
    "\n",
    "\n",
    "new_dataset = torch.empty(0,Crop_Window_Width, Crop_Window_Height)\n",
    "centers = []\n",
    "\n",
    "\n",
    "for i in range(0, dataset_size):\n",
    "    for j in range (0, crop_iteration):\n",
    "        img = two_class_dataset[i]  # 4 in range [0, 5842) ; 9 in range [5842, 11791)\n",
    "        img = Image.fromarray(img.numpy(), mode=\"L\")\n",
    "        a = confidence_crop_interspace(Input_Width, Input_Height, Crop_Window_Width, Crop_Window_Height)\n",
    "        centers.append((a[0][0], a[0][1]))\n",
    "        cropped_image = torchvision.transforms.functional.crop(torch.sum(transformation(img), 0), a[1][1], a[1][0], Crop_Window_Width, Crop_Window_Height)\n",
    "        cropped_image = cropped_image.view(1, Crop_Window_Width, Crop_Window_Height)\n",
    "        new_dataset = torch.cat((new_dataset.data, cropped_image.data), dim=0)\n",
    "\n",
    "dl = DataLoader(new_dataset,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = new_dataset[2] # 4 in range [0, 5842) ; 9 in range [5842, 11791)\n",
    "img = Image.fromarray(img.numpy(), mode=\"L\")\n",
    "show_image(torch.sum(transformation(img), 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
